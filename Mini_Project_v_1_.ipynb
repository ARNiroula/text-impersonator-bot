{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P5ZlgfJBeXn9"
      },
      "outputs": [],
      "source": [
        "file = open('./alice_in_wonderland.txt',encoding=\"utf8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rbH2vdhteXoA"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DXOFoZPQeXoB"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm',disable = ['parser','tagger','ner','lemmatizer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UA0SJrHHeXoB"
      },
      "outputs": [],
      "source": [
        "nlp.max_length = 1723200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WepA54t7eXoC"
      },
      "outputs": [],
      "source": [
        "def seperate_punch(doc_text):\n",
        "    return[ token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n •·']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jmNxNX40eXoC"
      },
      "outputs": [],
      "source": [
        "file = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uThcMhRfeXoD"
      },
      "outputs": [],
      "source": [
        "tokens = seperate_punch(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wYbjT1CWeXoE"
      },
      "outputs": [],
      "source": [
        "#Making lists of 11 words\n",
        "#We are going to set 10 as length of seed text and then going to predict 11th word\n",
        "train_len = 10 + 1\n",
        "text_seq = []\n",
        "#Creates list of lists of 11 words each\n",
        "for i in range(train_len,len(tokens)):\n",
        "    seq = tokens[i-train_len:i]\n",
        "    text_seq.append(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lElm9HO2eXoE"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FERKOW-QeXoF"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pjlCB2c0eXoG"
      },
      "outputs": [],
      "source": [
        "sequences  = tokenizer.texts_to_sequences(text_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gYIo-6pNeXoG"
      },
      "outputs": [],
      "source": [
        "#setting our vocaboulary size to total number of unique words we have\n",
        "vocab_size = len(tokenizer.word_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Cn-MT2ZDeXoG"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AOTzNHgSeXoH"
      },
      "outputs": [],
      "source": [
        "#Converting to numpy array\n",
        "sequences = np.array(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OgkFIcNieXoH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IM15d3CceXoH"
      },
      "outputs": [],
      "source": [
        "#X is our feature label\n",
        "#Taking first ten words form the array as our features\n",
        "X = sequences[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "NdN3tWWzeXoI"
      },
      "outputs": [],
      "source": [
        "#Taking last word of the array as our prediction \n",
        "y = sequences[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KZVh0m2heXoI"
      },
      "outputs": [],
      "source": [
        "#One-Hot-Encoding\n",
        "#num_classes = vocab_size+1 , one extra for padding\n",
        "y = to_categorical(y,num_classes=vocab_size+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zbHlav83eXoI"
      },
      "outputs": [],
      "source": [
        "#no of words in seed text\n",
        "seq_ln = 10 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Al9ugXzHeXoI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM,Embedding,Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "WLpcXQ8QeXoI"
      },
      "outputs": [],
      "source": [
        "#Modeldefination\n",
        "#Takes parameter vocab_size and length of seed text\n",
        "#Using one embedding layer, two LSTM layers and two dense layers\n",
        "#Printing summary and returning model\n",
        "def create_model(vocab_size,seq_ln):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size,seq_ln,input_length=seq_ln))\n",
        "    model.add(LSTM(256,return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(256))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(80,activation='relu'))\n",
        "    model.add(Dense(vocab_size,activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "v4OhKNbZeXoJ",
        "outputId": "c076b227-cee9-4b53-93c3-f2eea164b9ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 10, 10)            27050     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 10, 256)           273408    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 10, 256)           0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 80)                20560     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2705)              219105    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,065,435\n",
            "Trainable params: 1,065,435\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_model(vocab_size+1,seq_ln)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "YJKdQGvIeXoJ",
        "outputId": "23ab118b-24d1-47b6-f5d5-9d9e5c2c98b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/55\n",
            "465/465 - 11s - loss: 6.0440 - accuracy: 0.0736 - 11s/epoch - 23ms/step\n",
            "Epoch 2/55\n",
            "465/465 - 6s - loss: 5.7341 - accuracy: 0.0747 - 6s/epoch - 13ms/step\n",
            "Epoch 3/55\n",
            "465/465 - 6s - loss: 5.5987 - accuracy: 0.0798 - 6s/epoch - 13ms/step\n",
            "Epoch 4/55\n",
            "465/465 - 6s - loss: 5.4334 - accuracy: 0.0861 - 6s/epoch - 13ms/step\n",
            "Epoch 5/55\n",
            "465/465 - 6s - loss: 5.2858 - accuracy: 0.0941 - 6s/epoch - 13ms/step\n",
            "Epoch 6/55\n",
            "465/465 - 6s - loss: 5.1626 - accuracy: 0.1011 - 6s/epoch - 13ms/step\n",
            "Epoch 7/55\n",
            "465/465 - 6s - loss: 5.0578 - accuracy: 0.1095 - 6s/epoch - 13ms/step\n",
            "Epoch 8/55\n",
            "465/465 - 6s - loss: 4.9598 - accuracy: 0.1157 - 6s/epoch - 13ms/step\n",
            "Epoch 9/55\n",
            "465/465 - 6s - loss: 4.8607 - accuracy: 0.1246 - 6s/epoch - 13ms/step\n",
            "Epoch 10/55\n",
            "465/465 - 6s - loss: 4.9738 - accuracy: 0.1181 - 6s/epoch - 14ms/step\n",
            "Epoch 11/55\n",
            "465/465 - 6s - loss: 4.7601 - accuracy: 0.1310 - 6s/epoch - 13ms/step\n",
            "Epoch 12/55\n",
            "465/465 - 6s - loss: 4.6212 - accuracy: 0.1420 - 6s/epoch - 13ms/step\n",
            "Epoch 13/55\n",
            "465/465 - 6s - loss: 4.5281 - accuracy: 0.1482 - 6s/epoch - 14ms/step\n",
            "Epoch 14/55\n",
            "465/465 - 6s - loss: 4.4376 - accuracy: 0.1554 - 6s/epoch - 13ms/step\n",
            "Epoch 15/55\n",
            "465/465 - 6s - loss: 4.3537 - accuracy: 0.1615 - 6s/epoch - 13ms/step\n",
            "Epoch 16/55\n",
            "465/465 - 6s - loss: 4.2690 - accuracy: 0.1682 - 6s/epoch - 13ms/step\n",
            "Epoch 17/55\n",
            "465/465 - 6s - loss: 4.1903 - accuracy: 0.1704 - 6s/epoch - 13ms/step\n",
            "Epoch 18/55\n",
            "465/465 - 6s - loss: 4.1166 - accuracy: 0.1761 - 6s/epoch - 13ms/step\n",
            "Epoch 19/55\n",
            "465/465 - 6s - loss: 4.0375 - accuracy: 0.1825 - 6s/epoch - 13ms/step\n",
            "Epoch 20/55\n",
            "465/465 - 6s - loss: 3.9612 - accuracy: 0.1863 - 6s/epoch - 13ms/step\n",
            "Epoch 21/55\n",
            "465/465 - 6s - loss: 3.8928 - accuracy: 0.1923 - 6s/epoch - 13ms/step\n",
            "Epoch 22/55\n",
            "465/465 - 6s - loss: 3.8234 - accuracy: 0.1957 - 6s/epoch - 13ms/step\n",
            "Epoch 23/55\n",
            "465/465 - 6s - loss: 3.7484 - accuracy: 0.2002 - 6s/epoch - 13ms/step\n",
            "Epoch 24/55\n",
            "465/465 - 6s - loss: 3.6928 - accuracy: 0.2029 - 6s/epoch - 13ms/step\n",
            "Epoch 25/55\n",
            "465/465 - 6s - loss: 3.6305 - accuracy: 0.2115 - 6s/epoch - 13ms/step\n",
            "Epoch 26/55\n",
            "465/465 - 6s - loss: 3.5667 - accuracy: 0.2168 - 6s/epoch - 13ms/step\n",
            "Epoch 27/55\n",
            "465/465 - 6s - loss: 3.5060 - accuracy: 0.2221 - 6s/epoch - 13ms/step\n",
            "Epoch 28/55\n",
            "465/465 - 6s - loss: 3.4477 - accuracy: 0.2284 - 6s/epoch - 13ms/step\n",
            "Epoch 29/55\n",
            "465/465 - 6s - loss: 3.3910 - accuracy: 0.2324 - 6s/epoch - 13ms/step\n",
            "Epoch 30/55\n",
            "465/465 - 6s - loss: 3.3333 - accuracy: 0.2406 - 6s/epoch - 13ms/step\n",
            "Epoch 31/55\n",
            "465/465 - 6s - loss: 3.2780 - accuracy: 0.2480 - 6s/epoch - 13ms/step\n",
            "Epoch 32/55\n",
            "465/465 - 6s - loss: 3.2246 - accuracy: 0.2558 - 6s/epoch - 13ms/step\n",
            "Epoch 33/55\n",
            "465/465 - 6s - loss: 3.1779 - accuracy: 0.2600 - 6s/epoch - 13ms/step\n",
            "Epoch 34/55\n",
            "465/465 - 6s - loss: 3.1243 - accuracy: 0.2680 - 6s/epoch - 13ms/step\n",
            "Epoch 35/55\n",
            "465/465 - 6s - loss: 3.0705 - accuracy: 0.2742 - 6s/epoch - 13ms/step\n",
            "Epoch 36/55\n",
            "465/465 - 6s - loss: 3.0178 - accuracy: 0.2822 - 6s/epoch - 13ms/step\n",
            "Epoch 37/55\n",
            "465/465 - 6s - loss: 2.9692 - accuracy: 0.2889 - 6s/epoch - 13ms/step\n",
            "Epoch 38/55\n",
            "465/465 - 6s - loss: 2.9271 - accuracy: 0.2962 - 6s/epoch - 13ms/step\n",
            "Epoch 39/55\n",
            "465/465 - 6s - loss: 2.8820 - accuracy: 0.3058 - 6s/epoch - 13ms/step\n",
            "Epoch 40/55\n",
            "465/465 - 6s - loss: 2.8299 - accuracy: 0.3089 - 6s/epoch - 13ms/step\n",
            "Epoch 41/55\n",
            "465/465 - 6s - loss: 2.7835 - accuracy: 0.3174 - 6s/epoch - 13ms/step\n",
            "Epoch 42/55\n",
            "465/465 - 6s - loss: 2.7393 - accuracy: 0.3264 - 6s/epoch - 13ms/step\n",
            "Epoch 43/55\n",
            "465/465 - 6s - loss: 2.6921 - accuracy: 0.3338 - 6s/epoch - 13ms/step\n",
            "Epoch 44/55\n",
            "465/465 - 6s - loss: 2.6499 - accuracy: 0.3399 - 6s/epoch - 13ms/step\n",
            "Epoch 45/55\n",
            "465/465 - 6s - loss: 2.6105 - accuracy: 0.3437 - 6s/epoch - 13ms/step\n",
            "Epoch 46/55\n",
            "465/465 - 6s - loss: 2.5547 - accuracy: 0.3544 - 6s/epoch - 13ms/step\n",
            "Epoch 47/55\n",
            "465/465 - 6s - loss: 2.5190 - accuracy: 0.3638 - 6s/epoch - 13ms/step\n",
            "Epoch 48/55\n",
            "465/465 - 6s - loss: 2.4740 - accuracy: 0.3696 - 6s/epoch - 13ms/step\n",
            "Epoch 49/55\n",
            "465/465 - 6s - loss: 2.4322 - accuracy: 0.3777 - 6s/epoch - 13ms/step\n",
            "Epoch 50/55\n",
            "465/465 - 6s - loss: 2.3843 - accuracy: 0.3874 - 6s/epoch - 13ms/step\n",
            "Epoch 51/55\n",
            "465/465 - 6s - loss: 2.3461 - accuracy: 0.3953 - 6s/epoch - 13ms/step\n",
            "Epoch 52/55\n",
            "465/465 - 6s - loss: 2.3056 - accuracy: 0.4032 - 6s/epoch - 13ms/step\n",
            "Epoch 53/55\n",
            "465/465 - 6s - loss: 2.2694 - accuracy: 0.4071 - 6s/epoch - 13ms/step\n",
            "Epoch 54/55\n",
            "465/465 - 6s - loss: 2.2178 - accuracy: 0.4183 - 6s/epoch - 13ms/step\n",
            "Epoch 55/55\n",
            "465/465 - 6s - loss: 2.1825 - accuracy: 0.4223 - 6s/epoch - 13ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fca423088d0>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "#Fitting model on our data\n",
        "#Batch_size = 64 is just an arbitory number\n",
        "#Training upto 55 epochs\n",
        "#verbose = 2 (has values 0,1,2 for which type of summary you want while training your model)\n",
        "model.fit(X,y,batch_size=64,epochs=55,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "UKSZWmoaeXoK"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Using seed text to generate new text sequence\n",
        "#Genrating words equal to \"num_of_gen_words\"\n",
        "#using pad_seqences for cases where our seed text does not match our seed length of 10 words \n",
        "#pad_sequences truncates the word according to \"pre\" or \"post\" passed to it\n",
        "#Getting the word form tokenizer using the predicied word index\n",
        "#joining whole output and returning it\n",
        "\n",
        "def generated_text(model,tokenizer,seq_ln,seed_text,num_of_gen_words):\n",
        "    output_text = []\n",
        "    input_text = seed_text\n",
        "    for i in range(num_of_gen_words):\n",
        "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        pad_encoded = pad_sequences([encoded_text],maxlen=seq_ln,truncating='pre')\n",
        "        # predicted_wrd_index = model.predict_classes(pad_encoded,verbose=0)[0]\n",
        "        predicted_wrd_index =np.argmax(model.predict(pad_encoded,verbose=0),axis=1)[0]\n",
        "        predicted_wrd = tokenizer.index_word[predicted_wrd_index]\n",
        "        input_text += ' '+predicted_wrd\n",
        "        output_text.append(predicted_wrd)\n",
        "        \n",
        "    return ' '.join(output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "91fN-BW-eXoK"
      },
      "outputs": [],
      "source": [
        "# You can give a random seed text from training file itself like --> ' '.join(text_seq[70])\n",
        "#Or give custom seed text like -->\n",
        "seed_text = \"Alice became small and fell into the hatter rabbit hole\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "fqDvzpTTeXoK"
      },
      "outputs": [],
      "source": [
        "gen_text = generated_text(model,tokenizer,seq_ln,seed_text,num_of_gen_words=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "i-YAWU0GeXoL",
        "outputId": "cf0478c3-680c-4214-974d-c82bb593df4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed text : --- > Alice became small and fell into the hatter rabbit hole\n",
            "\n",
            "\n",
            "<<--------------------------------------->>\n",
            "\n",
            "\n",
            "Genrerated text : --- > the sounds noticed thatched with reply and sometimes edwin marched in the common moment she heard to tinkling a judge of idea so she went on among a immense curtain and washing it into the end of the edge of the country i 've none of it ' said alice ' i 've not gone in the sea ' ' i did n't remember what ' said alice ' why do you draw ' said the king ' i 'm glad i 'm mad ' ' i do n't know it ' pleaded with a tone ' and she squeezed\n"
          ]
        }
      ],
      "source": [
        "print(f\"Seed text : --- > {seed_text}\")\n",
        "print(\"\\n\\n<<--------------------------------------->>\\n\\n\")\n",
        "print(f\"Genrerated text : --- > {gen_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAeyQJ_XeXoL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Mini Project_v.1 .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}