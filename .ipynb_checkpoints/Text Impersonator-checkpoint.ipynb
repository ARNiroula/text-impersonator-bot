{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Points in the dataset are dependent on the other points in the dataset\n",
    "- Text is one of the most widespread forms of sequence data\n",
    "    - Understood as either a sequence of characters or a sequence of words\n",
    "    - common to work at the level of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to handle Sequential Data <br>\n",
    "- Since text aren't stateless and usually depends on some other parts of the text, feedforward neural networks can't effectively handle these types of data\n",
    "- Use Neural Network with internal memory i.e. Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To generate text, past input is needed \n",
    "    - Saving the output of a layer and feeding it to the input to predict the output of the next layer\n",
    "- RNN $\\rightarrow$ Class of artificial neural networks that is powerful for modelling sequence data such as time series or natural language "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it handles sequential information \n",
    "- Implementation of Internal Memory \n",
    "<img src = './images/rnn.png' width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://www.simplilearn.com/ice9/free_resources_article_thumb/Fully_connected_Recurrent_Neural_Network.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"x\" is the input layer, \"h\" is the hidden layer, and \"y\" is the output layer\n",
    "- \"A\",\"B\",\"C\" $\\rightarrow$ Network parameters used to improve the models\n",
    "    - At time \"t\" : \n",
    "        - Current Input $\\rightarrow$ Combination of input at x(t) and x(t-1)\n",
    "        - Output at any given time is fetched back to the network to improve on the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem with using Simple RNN for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vanishing Gradient Problem \n",
    "    - Arises when there is large sequence of data and/or there is multiple hidden layers in RNN\n",
    "    - Activation Function such as Sigmoid Function squishes a large input space into a small input space between 0 and 1; so, large change in input causes small change in the output\n",
    "    - Gradients carry information used in the RNN parameter update and when it becomes smaller and smaller, the parameter updates become insignificant and no real learning is done\n",
    "- To circumvent $\\rightarrow$ LSTM architecture  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Advanced version of RNN architecture to model Sequential data and their long-range dependencies better than conventional RNN\n",
    "- Hidden layer consisting of a gated cell which has four layers that interact with one another to produce the output of the cell along with its state\n",
    "- Gates limits the information that is passed through the cell and helps to determine which part of the information will be needed by the next cell and which part is to be discarded\n",
    "<img src = 'https://www.mdpi.com/remotesensing/remotesensing-12-00256/article_deploy/html/images/remotesensing-12-00256-g003.png' width = '400' height ='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At time \"t\" $\\rightarrow$ input vector $[h(t-1),x(t)]$ ; network cell state [c(t)] ; output vector [h(t)]\n",
    "- Hyperbolic tangent and Sigmoid activation functions\n",
    "- 3 gates control the cell states\n",
    "    - Forget Gate $\\rightarrow$ Controls what information in the cell state to forget given new information that entered the network <img src = 'https://miro.medium.com/max/711/1*PJ5atpFStpNWE_XpB4e8qQ.png' width = 300 height ='300'>\n",
    "    - Input Gate $\\rightarrow$ Controls what new information will be encoded into the cell state, given the new input information <img src = 'https://miro.medium.com/max/698/1*pAzAFns1ccuHmBvCqwh3Fg.png' width = 300 height ='300'>\n",
    "    - Output Gate $\\rightarrow$ Controls what information encoded in the cell state is sent to the network as input in the following time step via output vector h(t) <img src = 'https://miro.medium.com/max/715/1*wXoU29bsWxi1WQ0DUAnK7g.png' width = 300 height ='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it handles the vanishing gradient problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient contains the forget gate's activation that allows the LSTM to decide, at each time step, that certain information should not be forgotten and to update the model’s parameters accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How LSTM was implemented\n",
    "Using the Keras Sequential API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(#parameters))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(#parameters))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(#parameter,\n",
    "                activation='relu'))\n",
    "\n",
    "# Dropout for regularization\n",
    "model.add(Dropout(#value))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(#parameter,\n",
    "                activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Embedding` maps each input word to a n dimensional vector\n",
    "- `LSTM` cells; Since we are using 2 LSTM layer, return sequences is kept \n",
    "- Fully-connected `Dense` layer with `relu` activation; Adds additional representational capacity to the network\n",
    "- `Dropout` layer to prevent overfitting to the training data\n",
    "- `Dense` fully-connected output layer; Produces a probability for every word in the vocab using `softmax` activation\n",
    "- Compiled using `Adam` optimizer (a variant on Stochastic Gradient Descent) and trained using the `categorical_crossentropy` loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
